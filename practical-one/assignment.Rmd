---
title: "Lasso-Assignment"
output: html_notebook
---

Loading necessary packages.
```{r}
library(tidyverse)
library(caret)
library(glmnet)
library(ggplot2)
library(bestNormalize)
library(RColorBrewer)
library(splines)
library(gam)
```

Loading in and attaching the data.
```{r}
load("prostate2.Rdata")
attach(prostate)
```

Summary of the data and looking at interesting interactions.
```{r}
class(prostate)
str(prostate)
summary(prostate)
apply(prostate, 2, sd)
plot(prostate)

plot(lcavol, Cscore, main="Regression lcavol vs Cscore", xlab="lcavol", ylab="Cscore")
abline(lm(Cscore~lcavol), data=prostate, col='red')

plot(lpsa, Cscore, main="Regression lpsa vs Cscore", xlab="lpsa", ylab="Csore")
abline(lm(Cscore~lpsa), data=prostate, col='red')

plot(lweight, Cscore, main="Regression lweight vs Cscore", xlab="lweight", ylab="Csore")
abline(lm(Cscore~lweight), data=prostate, col='red')

plot(lbph, Cscore, main="Regression lbph vs Cscore", xlab="lbph", ylab="Csore")
abline(lm(Cscore~lbph), data=prostate, col='red')

plot(age, Cscore, main="Regression age vs Cscore", xlab="age", ylab="Csore")
abline(lm(Cscore~age), data=prostate, col='red')

plot(lcp, Cscore, main="Regression lcp vs Cscore", xlab="lcp", ylab="Csore")
abline(lm(Cscore~lcp), data=prostate, col='red')
```

Preparing data for model.
```{r}
svi = as.factor(svi)         # Setting svi to categorical variable

x=model.matrix(Cscore~., prostate)[,-1]         # Formatting data for glmnet
y=Cscore
```



```{r}
hist(y, prob=TRUE, main='Density plot for Cscore', breaks=50, col='grey', xlim=c(-50,250), ylim = c(0, 0.025), xlab='Cscore')
lines(density(y, adjust=0.8), col="blue", lwd=2)
```

```{r}
set.seed(1)                                     # Splitting into training and test data
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
```

```{r}
grid=10^seq(10, -2, length=100)
lasso.mod=glmnet(x[train,], y[train], alpha=1, lambda=grid)

plot(lasso.mod, xvar="lambda", col = brewer.pal(n = 7, name = "Dark2"))
legend("bottomright", lwd = 1, col = brewer.pal(n = 7, name = "Dark2"), legend = colnames(x[train,]))
```

Using cross-validation to find the best lambda and then
```{r}
set.seed(1)
cv.out=cv.glmnet(x[train,], y[train], alpha=1, family='gaussian')
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
lasso.pred=predict(lasso.mod, s=bestlam, newx=x[test,])
mean((lasso.pred-y.test)^2)
```

```{r}
out=glmnet(x, y, alpha=1, lambda=grid)
lasso.coef=predict(out, type="coefficients", s=bestlam) [1:8,]
lasso.coef
```
LASSO regression for a normalised response variable.
```{r}
prostate1 <- prostate
prostate1$svi <- as.factor(prostate1$svi)

x1 <- model.matrix(Cscore~., prostate1)[,-1]
y1 <- prostate1$Cscore

BNobject <- bestNormalize(y1)

hist(BNobject$x.t, prob=TRUE, main='Density plot for Cscore', breaks=50, col='grey', xlim=c(-4,4), ylim=c(0,0.5), xlab='Cscore')
lines(density(BNobject$x.t, adjust=2), col="blue", lwd=2)
```

```{r}
set.seed(1)                                     # Splitting into training and test data
train1=sample(1:nrow(x), nrow(x)/2)
test1=(-train1)
y1.test=y[test]
```


```{r}
grid=10^seq(10, -2, length=100)
lasso.mod=glmnet(x1[train1,], BNobject$x.t[train1], alpha=1, lambda=grid, family='gaussian')

plot(lasso.mod, xvar="lambda", col = brewer.pal(n = 7, name = "Dark2"))
legend("bottomright", lwd = 1, col = brewer.pal(n = 7, name = "Dark2"), legend = colnames(x[train,]))
```

```{r}
set.seed(1)
cv.out=cv.glmnet(x1[train1,], BNobject$x.t[train1], alpha=1, family='gaussian')
plot(cv.out)
bestlam =cv.out$lambda.min
bestlam
lasso.pred.transformed = predict(lasso.mod, s=bestlam, newx=x1[test1,])
lasso.pred = predict(BNobject, newdata=lasso.pred.transformed, inverse=TRUE)
mean((lasso.pred-y1.test)^2)
```
```{r}
out=glmnet(x, BNobject$x.t, alpha=1, lambda=grid, family='gaussian')
lasso.coef=predict(out, type="coefficients", s=bestlam) [1:8,]
lasso.coef
```
  
Fitting a model with appropriate non-linear effects.

```{r}
lcavol.lims = range(lcavol)
plot(lcavol, Cscore, xlim=lcavol.lims, cex =.5, col="darkgrey")
title("Smoothing Spline for LCAVOL")
lcavol.fit = smooth.spline(lcavol, Cscore, cv=TRUE)
lcavol.df = lcavol.fit$df
lcavol.lambda = lcavol.fit$lambda
lines(lcavol.fit, col="purple", lwd=2)
legend("topright", legend="3.84 DF", col="purple", lty=1, lwd=2, cex=.8)

lpsa.lims = range(lpsa)
plot(lpsa, Cscore, xlim=lpsa.lims, cex =.5, col="darkgrey")
title("Smoothing Spline for LPSA")
lpsa.fit = smooth.spline(lpsa, Cscore, cv=TRUE)
lpsa.df = lpsa.fit$df
lpsa.lambda = lpsa.fit$lambda
lines(lpsa.fit, col="purple", lwd=2)
legend("topright", legend="4 DF", col="purple", lty=1, lwd=2, cex=.8)
```

```{r}
set.seed(1)                                     # Splitting into training and test data
smp_size = floor(0.8 * nrow(prostate))
train_ind = sample(seq_len(nrow(prostate)), size = smp_size)

gamtrain = prostate[train_ind, ]
gamtest = prostate[-train_ind, ]
```

Fitting and testing GAMs using the smoothing splines.
```{r}
gam1 = gam(Cscore~s(lpsa, lpsa.df) + s(lcavol, lcavol.df) + svi + lweight + lcp, data=gamtrain)
gam1.predict = predict(gam1, newdata=gamtest)
gam1.MSE = mean((gam1.predict - gamtest$Cscore)^2)

gam2 = gam(Cscore~s(lpsa, lpsa.df) + s(lcavol, lcavol.df) + svi + age + lweight + lcp, data=gamtrain)
gam2.predict = predict(gam2, newdata=gamtest)
gam2.MSE = mean((gam2.predict - gamtest$Cscore)^2)

gam3 = gam(Cscore~s(lpsa, lpsa.df) + s(lcavol, lcavol.df) + svi + lbph + lweight + lcp, data=gamtrain)
gam3.predict = predict(gam3, newdata=gamtest)
gam3.MSE = mean((gam3.predict - gamtest$Cscore)^2)

gam4 = gam(Cscore~s(lpsa, lpsa.df) + s(lcavol, lcavol.df) + svi + age + lbph + lweight + lcp, data=gamtrain)
gam4.predict = predict(gam4, newdata=gamtest)
gam4.MSE = mean((gam4.predict - gamtest$Cscore)^2)

gam1.MSE
gam2.MSE
gam3.MSE
gam4.MSE
```

Anova for the different models.
```{r}
anova(gam1, gam2, gam3, gam4, test="F")
```