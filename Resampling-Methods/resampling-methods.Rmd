---
title: "Resampling Methods"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 2
    number_sections: true
---
# Installing packages
```{r, results="hide"}
install.packages('ISLR')
install.packages('boot')
```

# The Validation Set Approach
```{r}
library(ISLR)
attach(Auto)
set.seed(1)
train=sample(392,196)

lm.fit=lm(mpg~horsepower, data=Auto, subset=train)
```

We now use the predict() function to estimate the response for all 392 observations, and we use the mean() function to calculate the MSE of the 196 observations in the validation set. Note that the -train index below selects only the observations that are not in the training set.

```{r}
mean((mpg-predict(lm.fit ,Auto))[-train ]^2)
```

We can use the poly() function to estimate the test error for the quadratic and cubic regressions.

```{r}
lm.fit2 = lm(mpg~poly(horsepower, 2), data=Auto, subset=train)
mean((mpg-predict(lm.fit2, Auto))[- train]^2)
lm.fit3 = lm(mpg~poly(horsepower, 3), data=Auto, subset=train)
mean((mpg-predict(lm.fit3, Auto))[- train]^2)
```

# Leave Out One Cross Validation
```{r}
library(boot)
glm.fit=glm(mpg~horsepower, data=Auto)
cv.err=cv.glm(Auto, glm.fit)
cv.err$delta
```

The cv.glm() function produces a list with several components. The two numbers in the delta vector contain the cross-validation results. We can repeat this procedure for increasingly complex polynomial fits.

```{r}
cv.error = rep(0,5)
for (i in 1:5){
glm.fit = glm(mpg~poly(horsepower, i), data=Auto)
cv.error[i] = cv.glm(Auto, glm.fit)$delta [1]
}
cv.error
```

## K-Fold Cross Validation

The cv.glm() function can also be used to implement k-fold CV. Below we use k = 10, a common choice for k, on the Auto data set. We once again set a random seed and initialize a vector in which we will store the CV errors corresponding to the polynomial fits of orders one to ten.

```{r}
set.seed(17)
cv.error.10 = rep(0,10)
for (i in 1:10){
glm.fit = glm(mpg~poly(horsepower, i), data=Auto)
cv.error.10[i] = cv.glm(Auto, glm.fit, K=10)$delta[1]
}
cv.error.10
```

# The Bootstrap
## Estimating the Accuracy of a Linear Regression Model

Here we use the bootstrap approach in order to assess the variability of the estimates for β0 and β1, the intercept and slope terms for the linear regression model that uses horsepower to predict mpg in the Auto data set.

We first create a simple function, boot.fn(), which takes in the Auto data set as well as a set of indices for the observations, and returns the intercept and slope estimates for the linear regression model. We then apply this 
function to the full set of 392 observations in order to compute the estimates of β0 and β1 on the entire data set.

```{r}
boot.fn = function(data, index)
return(coef(lm(mpg~horsepower, data=data, subset=index)))
boot.fn(Auto, 1:392)
```

The boot.fn() function can also be used in order to create bootstrap estimates for the intercept and slope terms by randomly sampling from among the observations with replacement. Here we give two examples.

```{r}
set.seed(1)
boot.fn(Auto, sample(392,392,replace=T))
boot.fn(Auto, sample(392,392,replace=T))
```

Next, we use the boot() function to compute the standard errors of 1,000 bootstrap estimates for the intercept and slope terms.

```{r}
boot(Auto ,boot.fn ,1000)
```

Lets compare the standard erros with those obtained from the summary function

```{r}
summary(lm(mpg~horsepower, data=Auto))$coef
```

Interestingly, these are somewhat different from the estimates obtained using the bootstrap. Does this indicate a problem with the bootstrap? In fact, it suggests the opposite. Bootstrap makes fewer assumptions about the distribution of variance among different components in the model. Below we compute the bootstrap standard error estimates and the stan dard linear regression estimates that result from fitting the quadratic model to the data.

```{r}
boot.fn = function(data, index)
coefficients(lm(mpg~horsepower+I(horsepower^2), data=data, subset=index))
set.seed(1)
boot(Auto, boot.fn, 1000)
```

```{r}
summary(lm(mpg~horsepower+I(horsepower^2), data=Auto))$coef
```

